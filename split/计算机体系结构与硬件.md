# 计算机体系结构与硬件

## 冯诺依曼与哈弗体系结构

冯‘诺依曼体系：计算机硬件由运算器、控制器、存储器、输入设备和输出设备五大部分组成

|         体系         |   冯诺依曼    |                哈佛                |                    改进的哈佛（现代ARM）                     |
| :------------------: | :-----------: | :--------------------------------: | :----------------------------------------------------------: |
|  数据与程序存储方式  |  存储在一起   |              分开存储              |                           分开存储                           |
|     CPU总线条数      | 1*(地址+数据) |           2*(地址+数据)            | 1*(地址+数据)（新增cache，cpu由1条总线读cache，cache有2条总线） |
| 取指操作与取数据操作 |     串行      |           并行，可预取指           |                        并行，可预取指                        |
|         缺点         |    成本低     |               成本高               |                             综合                             |
|         优点         |  执行效率低   | 效率高，流水线（取指、译码、执行） |                            同哈佛                            |

<img src="嵌入式笔记.assets/SouthEast.png" alt="这里写图片描述" style="zoom: 33%;" /><img src="https://obsidian-1321127127.cos.ap-beijing.myqcloud.com/SouthEast-166701102120110.png" alt="这里写图片描述" style="zoom: 33%;" /><img src="https://obsidian-1321127127.cos.ap-beijing.myqcloud.com/SouthEast-166701103111813.png" alt="这里写图片描述" style="zoom: 33%;" />



## ARM架构与x86架构区别

最主要区别：指令集

- ARM：精简指令集RISC
- X86：复杂指令集CISC

功耗

- ARM：主要面向低功耗
- X86：通过制程弥补功耗劣势

性能

ARM：低性能，顺序执行能力强，流水线指令集，主频低于1G

X86：高性能，乱序执行能力强，主频高



## 流水线

CPU的流水线（Pipeline）是一种提高处理器执行效率的技术，将指令执行过程划分为多个阶段，并使多个指令在不同阶段之间并行执行，从而实现指令级并行。

CPU流水线通常包括以下几个阶段：

1. 取指（Instruction Fetch）：从内存中获取下一条指令。
2. 译码（Instruction Decode）：将指令解析成对应的操作码和操作数，并为执行阶段做准备。
3. 执行（Execute）：执行指令的具体操作，如算术运算、逻辑运算等。
4. 访存（Memory Access）：如果指令需要访问内存，这个阶段用于进行数据的读取或写入操作。
5. 写回（Write Back）：将执行结果写回到寄存器中，更新寄存器的内容。

每条指令在流水线中按顺序通过不同的阶段，形成一个连续的流水线操作。当一个指令完成当前阶段的操作后，就会进入下一阶段，同时下一条指令进入到当前阶段，从而实现指令的并行执行。

通过流水线技术，CPU可以实现更高的处理能力和更好的性能指标，因为在同一时钟周期内可以同时执行多个指令。然而，流水线也会引入一些问题，如流水线的阻塞、冲突和分支预测问题，可能导致流水线效率下降。为了解决这些问题，还可以采取一些技术手段，如超标量流水线、动态调度、乱序执行等。



一个任务执行阶段，开始下一个任务的取指、译码阶段

- 提高了吞吐量，但单任务的执行时间没有减少
- 受制于最慢的流水线
- 对程序员不可见

RISC5级流水线步骤

1. 取指（访问Icache得到PC）
2. 译码（翻译指令并从寄存器取数）
3. 执行（运算）
4. 访存（访问存储器，读取操作数）（4级流水线独有）
5. 写回（将结果写回寄存器）（5级流水线独有）

ARM3级流水线步骤

1. 取指
2. 译码
3. 执行





## CPU、MCU、SOC区别

- CPU：运算器、控制器、寄存器组成，主要负责取指、放入寄存器、译码、执行指令并更新寄存器（仅存在理论之中）
- MPU：增强版的CPU
- MCU：CPU+RAM+ROM+I/O，在CPU的基础上加入片上RAM、Flash、串口、ADC等外设，在一块芯片上集成整个计算机系统
- SOC：MPU+RAM+ROM+I/O+特定功能模块（如电能计量、编解码），将MPU的计算能力和MCU的外设结合



## Cache

高速     中等速度   低速

CPU <------> Cache <-----> RAM

Cache，就是一种缓存机制，它位于CPU和DDR RAM之间，为CPU和DDR之间的读写提供一段内存缓冲区。cache一般是SRAM，它采用了和制作CPU相同的半导体工艺，它的价格比DDR要高，但读写速度要比DDR快不少。例如CPU要执行DDR里的指令，可以一次性的读一块区域的指令到cache里，下次就可以直接从cache里获取指令，而不用反复的去访问速度较慢的DDR。又例如，CPU要写一块数据到DDR里，它可以将数据快速地写到cache里，然后手动执行一条刷新cache的指令就可以将这片数据都更新到DDR里，或者干脆就不刷新，待cache到合适的时候，自己再将内容flush到DDR里。总之一句话，cache的存在意义就是拉近CPU和DDR直接的性能差异，提高整个系统性能。

Cache分为I-Cache（指令缓存）与D-Cache（数据缓存）

![20210528111327990.png](https://obsidian-1321127127.cos.ap-beijing.myqcloud.com/20210528111327990.png)

cache是多级的，在一个系统中你可能会看到L1、L2、L3, 当然越靠近core就越小，也是越昂贵。

CPU接收到指令后，它会最先向CPU中的一级缓存（L1 Cache）去寻找相关的数据，然一级缓存是与CPU同频运行的，但是由于容量较小，所以不可能每次都命中。这时CPU会继续向下一级的二级缓存（L2 Cache）寻找，同样的道理，当所需要的数据在二级缓存中也没有的话，会继续转向L3 Cache、内存(主存)和硬盘.

![9a20a239133a43e8b307e2ac08e7db8b.png](https://obsidian-1321127127.cos.ap-beijing.myqcloud.com/9a20a239133a43e8b307e2ac08e7db8b.png)

不能使用cache的情况

1. CPU读取外设的内存数据，如果外设的数据本身会变，如网卡接收到外部数据，那么CPU如果连续2次读外设的操作相差时间很短，而且访问的是同样的地址，上次的内存数据还存在于cache当中，那么CPU第二次读取的可能还是第一次缓存在cache里数据。
2. CPU往外设写数据，如向串口控制器的内存空间写数据，如果CPU第1次写的数据还存在于cache当中，第2次又往同样的地址写数据，CPU可能就只更新了一下cache，由cache输出到串口的只有第2次的内容，第1次写的数据就丢失了。
3. 在嵌入式开发环境中，经常需要在PC端使用调试工具来通过直接查看内存的方式以确定某些事件的发生，如果定义一个全局变量来记录中断计数或者task循环次数等，这个变量如果定义为cache的，你会发现有时候系统明明是正常运行的，但是这个全局变量很长时间都不动一下。其实它的累加效果在cache里，因为没有人引用该变量，而长时间不会flush到DDR里
4. 考虑双cpu的运行环境(不是双核)。cpu1和cpu2共享一块ddr，它们都能访问,这块共享内存用于处理器之间的通信。cpu1在写完数据到后立刻给cpu2一个中断信号，通知cpu2去读这块内存，如果用cache的方法，cpu1可能把更新的内容只写到cache里，还没有被换出到ddr里，cpu2就已经跑去读，那么读到的并不是期望的数据。

![image-20230607151630997.png](https://obsidian-1321127127.cos.ap-beijing.myqcloud.com/image-20230607151630997.png)



## 为何启动时关闭Cache

在嵌入式系统和某些应用程序中，启动时关闭指令缓存（Instruction Cache）和数据缓存（Data Cache）是一种常见的做法。以下是一些原因：

1. 避免缓存冲突：在启动阶段，代码和数据通常是从外部存储器（如闪存）加载到内部存储器（如RAM）中。由于这些加载过程往往涉及重复的读写操作，启动时关闭缓存可以防止缓存中的“旧”数据对加载过程产生冲突，确保正确加载并执行新的代码和数据。
2. 简化启动过程：在关闭缓存的情况下，处理器将直接从内存中读取指令和数据，而不依赖于缓存。这样可以避免额外的缓存管理开销，并简化启动代码的编写和调试过程。
3. 确保数据的一致性：某些应用程序要求数据在内存和外部设备之间保持一致。在关闭缓存的情况下，每次访问数据都将直接从内存取，确内存中的数据始终与外部设备保持一致，关闭存并不适用于所有应用场景，并且可能会对性能产生负面影响。在实际应用中，应根据具体的系统需求和性能要求来决定是否关闭缓存。



## 存储器层次结构与分类

![20210528110828244.png](https://obsidian-1321127127.cos.ap-beijing.myqcloud.com/20210528110828244.png)


